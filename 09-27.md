# Notes for the 9/27 lecture

### Moore's Law

Since 1940
We have used this perfomance to make computers easier to use

### Processor design trends

### Brick Wall

## Throughput

Making processors better at doign many things at once rather than one task faster

- Refocus on thread throughput over latency
  - Simultaneous Multi-Threading (SMT)
    - hyper threading
  - Single Chip Multi-Processor (CMP)
    - muti-core

## The rise of parallelism

- Multi-processors
  - If one CPUT is fast, 2 must be faster
  - They allow you to double "performance" without changing the clock speed
- Seems simple, so why are we doing this right now?
  - Speeding up a single CPU makes everythig faster!
    - An application's performance double every 18 months with no effort on the programmer's part.
  - Getting perfomrance out of a multiprocessor requires work
    - Parallelizing code is difficult
    - There aren't that many threads

## Modern CPUs (to scale)

## Specialization

- Single cores are to slow
- Multi-core is hard to program and inefficient
- Some applications are worth specializing
  - Graph
  - AI
  - Video processing
  - Supercomputing
- So build hardware specifcally for those domains

# Measuing Performnace

## Match (Best) Performacne Metric to Domain

Performance Metrics

- Network Bandwidth (MB/sec)
- Network Latency (ms)
- Frame Rate (frames/sec)
- Throughput (ops/sec)

## Latency: How long does it take

- Measured in seconds (per opration
  - A CPU doesn’t have a latency
- A particular task on a particular CPU has a latency
  - Why we care about latency:
- When someone is waiting
- If it impacts human perception
- When something needs to be done within a fixed time
  - Related metrics
- “speed” and “performance” often mean the inverse of latency
  (1/latency) (but not always)

## Bandwidth

- “Bandwidth” (or “throughput”) measures “work” per time
  - ”Work” can many things – bytes, instructions, function calls, etc.
- Bytes/Second – Memory bandwidth, network bandwidth
- Instructions/Second – Instruction bandwidth
- Matrix-multiplies/Second – Execution bandwidth
  - Bandwidth is related to latency
- Latency = Work/Bandwidth
- e.g., The latency required to write 1MB to a disk at 10MB/s is 1/10 = 0.1s.

> Question: I have a program that
>
> - runs in 10 seconds on my old computer
> - 5 seconds on my new computer
>   How much faster is my new computer?
>
> **100%**

## Speedup: How Much Faster is It?

• ”Speed up” measures the change in latency
– B = latency before a change (e.g., an optimization)
– A = latency after the change
– Speedup = S = B/A
• Speedup can be < 1, if the change hurt performance
– but we can reverse this to say the older machine/pre-change
is 1.x faster than the new machine/post-change.

## Three Key Equations In Computer Architecture

These are absolutely essential knowledge:

- Amdahl’s Law
- The Performance Equation
- The Power Equation

These equations are central to understanding

- How fast computers are
- How efficient they are
- How we can improve their performance and efficiency
  We will use them a lot

## Amdahl’s Law: The Fundamental Theorem of Performance Optimization

## Amdahl’s Law and Parallelism

> Our program is 90% parallelizable (segment of code executable in parallel on
> multiple cores) and runs in 100 seconds with a single core.
>
> What is the execution time if you use 4 cores (assume no overhead for
> parallelization)?

> 90/4 + 10 = 22.5+10 = 32.5

## Speedup vs. Sizeup

- Speedup runs into problem for parallelization because of diminishing returns and dominance of serial execution
- What if there were a constant?
-
