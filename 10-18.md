## Summary three types of cache miss

Compulsory (or cold-start) misses

- first access to the data.

Capacity misses

- we missed only because the cache isn’t big enough.
- Defn: The address has already been requested AND a fully
  associative cache of the same size would also experience a
  miss

Conflict misses

- we missed because of the indexing scheme/addresses caused
  one block to evict another earlier than one might hope
- Defn: The address has already been requested AND a fully
  associative cache of the same size would experience a hit

## Increase block size to 64 bytes would be helping because dexrease the compulsory misses

##

```c
for(int i=0; i<10,000,000;i++)
for(int j = 0; j<8192;j++)

sum+= A[j] – B[j];
```

Assume each element of A and B are 4 bytes and each array is at least 32KB
in size. Assume sum is kept in a register. Assume a baseline direct-mapped
32KB L1 cache with 32 byte blocks. Which changes would help the hit rate of
the above code?

Selection Change

- A Increase to 2-way set associativity (capacity miss)
- B Increase block size to 64 bytes (compulsory miss)
- C Increase cache size to 64 KB (conflict miss)
- D A and C combined
- E A, B, and C combined

> A Increase associatively not going to help, even to full, doesn't increase capacity

> B increase compulsory misses

> C： don't know the adddresses of A and B, conflicts based off the indexes,

> divide into 64 kb chunks
> map on to the same index in the cache, cache lines are going to conflict, fight over the single slot.

## Assume a 2-way Single Associative 16KB L1 cache with 32 byte blocks.

```c
for(int i=0; i<10,000,000;i++)
for(int j = 0; j<8192;j++)
sum+= A[j] – B[j];
```

> Flip the loops

> 8192: 0~2048, 2048~4096, etc

> `2048*2*4 = 16 kb `, 2 is A,B to ints, and 4 is 4 bytes per int

Thinking about Associativity and Block size from a
performance perspective:

- What are the benefits/costs to higher associativity?

  - Cache access times increase
  - cycle time suffers

- What are the benefits/costs to block size?
  - good if thre is patial locality, bad if there isn't

## Let's add an int (4 byte) array C with 2 elements. A and B are int (4 byte) arrays with 2 elements. Assume i, j, and sum are all in registers.

- A's base address is: 0x0000 0010
- B's base address is: 0x0000 0030
- C's base address is: 0x0000 0050

```c
for(int i = 0; i < 2; i++)
    for(int j = 0; j < 4; j++)
        C[j] = A[j] - B[j]
```

What is the new stream of addresses?

- Assume we access A, then B, then C

## Handling Stores

Fundamentally stores are different than loads

- They are non-blocking (you can just write to a store buffer)

You don't need the data in the block to read

- We have separate policies for how to handle stores
- Policies vary by cache level

## Handling Stores - Policies

On a store miss, we can:

- Write allocate: Bring the cache block into cache (then use the
  store hit policy)
- Write no-allocate: Just notify lower levels of cache about what was changed

On a store hit, we can:

- Write through: tell lower levels cache what was changed immediately
- Write back: mark the block as dirty and tell lower levels of cache about the change when the block is evicted

## Handling Stores – Policy by cache level

- L1 may be write-through and write allocate
- High bandwidth to L2 + may wish to keep L2 consistent
- L2 will likely be write-back and write allocate

- Bandwidth limited on store hits, likely store locality so write-
  allocate

You will never see L3 or Memory (just a cache for disk)
as write-through

- Bandwidth is too limited

## How does all this impact our performance equation?

Suppose your processor finishes one instruction per cycle without any memory hazards (i.e., misses). Also, your L1 has a 20 cycle miss penalty to memory and an I-cache miss rate of 2% and a D-cache miss rate of 5%. You
can assume a non-blocking store queue and 20% of your instructions are
loads. What part of the performance equation is impacted and what is the
new value?

```
ICache: instruction cache holds instruction
Data Cache: data cache holds data,
   ----> access only for loads and stores


ET = IC*CPI*CT

L1： 20 cycle miss penalty
l cache: 2% miss rate
D cache: 5% miss rate

20%instructions note

Base CPI: 1
CPI: 1 + 2%*20 cycles + 20% * 5% * 20 cycles = 1.6 cycles per insructions
```

## Prefetching

“Watch the trends in movie watching and attempt to
guess movies that will be rented soon – put those in the
front office.”

- Can be done either with
  - Hardware Prefetching
  - Software Prefetching

## Hardware Prefetching

## Costs of Hardware Prefetching?

• Bandwidth

- You are using "idle" bandwidth but a prefetch request may
  still delay other requests if you let them use the bus

• Energy

- Every access to lower levels of cache/memory costs energy.
  You need to be correct often to make it worthwhile

## Software Prefetching

- PREFETCHh, (%eax)

- Prefetch pulls data into cache hoping it is used later.

- You can insert your own prefetches. For example, if

you have an odd access pattern but could pre-
compute an address coming sometime soon, you can
put in a prefetch instruction
